Github-[MAOYUQI](yuki-820/My-project-code-and-independent-research: This repository stores all the project and independent research codes and essays mentioned in my resume)

åŸå§‹æ•°æ®æ¥æºï¼šä¸œæ–¹è´¢å¯Œå§ï¼ŒRESSETæ•°æ®åº“

## ğŸ“Œ é¡¹ç›®ä»‹ç»
æœ¬é¡¹ç›®æ—¨åœ¨åˆ©ç”¨ **æ··åˆé›†æˆå­¦ä¹ æ–¹æ³•** å¯¹ç§‘åˆ›æ¿å…¬å¸è‚¡ç¥¨çš„æœªæ¥æ”¶ç›Šç‡è¿›è¡Œé¢„æµ‹ã€‚  
ç ”ç©¶ç›®æ ‡æ˜¯ç»“åˆ **æŠ€æœ¯æŒ‡æ ‡** ä¸ **å¸‚åœºæƒ…ç»ªå› å­**ï¼Œåœ¨å•æ¨¡å‹ä¸é›†æˆç­–ç•¥çš„åŸºç¡€ä¸Šï¼Œæé«˜è‚¡ç¥¨æ¶¨è·Œæ–¹å‘é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚  

æ¨¡å‹åœ¨ç§‘åˆ›æ¿ä¸¤å®¶å…¬å¸çš„æ•°æ®ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶åœ¨éªŒè¯é›†ä¸Šé€‰æ‹©æœ€ä¼˜é›†æˆæ–¹å¼ï¼Œæœ€ç»ˆåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†æ”¶ç›Šç‡é¢„æµ‹ä¸å¯è§†åŒ–ã€‚

---

## ğŸ“Š æ•°æ®ä¸ç‰¹å¾

### 1. æŠ€æœ¯æŒ‡æ ‡
#### (1) MACD æŒ‡æ ‡
- çŸ­æœŸ EMA (12æ—¥)ï¼š  
  $$
  EMA_{12}(t) = \alpha \times ClPr_t + (1 - \alpha) \times EMA_{12}(t-1), \quad \alpha = \frac{2}{1+12}
  $$
- é•¿æœŸ EMA (26æ—¥)ï¼š  
  $$
  EMA_{26}(t) = \beta \times ClPr_t + (1 - \beta) \times EMA_{26}(t-1), \quad \beta = \frac{2}{1+26}
  $$
- DIF:  
  $$
  DIF(t) = EMA_{12}(t) - EMA_{26}(t)
  $$
- DEA:  
  $$
  DEA(t) = \gamma \times DIF(t) + (1 - \gamma) \times DEA(t-1), \quad \gamma = \frac{2}{1+9}
  $$
- MACD:  
  $$
  MACD(t) = 2 \times (DIF(t) - DEA(t))
  $$

#### (2) KDJ æŒ‡æ ‡
- RSV:  
  $$
  RSV(t) = \frac{ClPr_t - LowestLow(t)}{HighestHigh(t) - LowestLow(t)} \times 100
  $$
- K:  
  $$
  K(t) = \delta \times RSV(t) + (1 - \delta) \times K(t-1), \quad \delta = \frac{2}{1+3}
  $$
- D:  
  $$
  D(t) = \epsilon \times K(t) + (1 - \epsilon) \times D(t-1), \quad \epsilon = \frac{2}{1+3}
  $$
- J:  
  $$
  J(t) = 3K(t) - 2D(t)
  $$

#### (3) ATR æ³¢åŠ¨æ€§æŒ‡æ ‡
- çœŸå®æ³¢å¹… (TR):  
  $$
  TR(t) = \max(|HiPr_t - LoPr_t|, |HiPr_t - PrevClPr_t|, |LoPr_t - PrevClPr_t|)
  $$
- å¹³å‡çœŸå®æ³¢å¹… (ATR):  
  $$
  ATR(t) = \frac{\sum_{i=t-14}^{t} TR(i)}{14}
  $$

### 2. é«˜é¢‘äº¤æ˜“ç‰¹å¾
- `å‘¨æ³¢åŠ¨ç‡(%)_VolatilityWk`  
- `æ¢æ‰‹ç‡(%)_TurnRat_diff`  
- `å‘¨æ¢æ‰‹ç‡(%)_TurnRatRecWk_diff`  
- `æˆäº¤é‡(è‚¡)_diff`  
- ä»¥ä¸Šåç¼€ä¸º_diffçš„ç‰¹å¾å‡é‡‡ç”¨å·®åˆ†å½¢å¼ã€‚

### 3.è‚¡æ°‘è¯„è®ºæ•°æ®å¤„ç†ä¸æƒ…ç»ªå‘é‡é™ç»´

 åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸œæ–¹è´¢å¯Œç½‘çš„è‚¡æ°‘è¯„è®ºæ•°æ®æ¥æ„å»ºå¸‚åœºæƒ…ç»ªç‰¹å¾ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå®Œæ•´çš„å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬è¯„è®ºæ•°æ®çš„è¯å‘é‡åŒ–ã€æ—¶é—´è¡°å‡åŠ æƒã€é™ç»´å¤„ç†ç­‰ç¯èŠ‚ï¼Œæœ€ç»ˆç”Ÿæˆæ¯æ—¥çš„å¸‚åœºæƒ…ç»ªç‰¹å¾å‘é‡ã€‚
#### è¯„è®ºæ•°æ®çš„é¢„å¤„ç†ä¸è¯å‘é‡åŒ–

æˆ‘ä»¬é¦–å…ˆå¯¹è‚¡æ°‘è¯„è®ºæ•°æ®è¿›è¡Œåˆ†è¯ã€å»åœç”¨è¯å¤„ç†ï¼Œç„¶åè®­ç»ƒ **Word2Vec** æˆ– **FastText** æ¨¡å‹ï¼Œå°†æ¯æ¡è¯„è®ºè½¬æ¢ä¸ºè¯å‘é‡çš„å‡å€¼è¡¨ç¤ºï¼š
```python
import jieba
import numpy as np
import pandas as pd
from gensim.models import Word2Vec, FastText

# åˆ†è¯åŠå»åœç”¨è¯
stop_words = set(["çš„", "äº†", "æ˜¯", "å’Œ", "ä¹Ÿ", "éƒ½", "å°±", "åœ¨"])
def tokenize(text):
    return [word for word in jieba.lcut(text) if word not in stop_words]

df["tokenized"] = df["comment"].astype(str).apply(tokenize)

# è®­ç»ƒ Word2Vec æˆ– FastText
use_fasttext = False
if use_fasttext:
    model = FastText(sentences=df["tokenized"].tolist(), vector_size=100, window=5, min_count=2, workers=4)
else:
    model = Word2Vec(sentences=df["tokenized"].tolist(), vector_size=100, window=5, min_count=2, workers=4)

# è·å–å‡å€¼è¯å‘é‡
def get_mean_vector(words, model):
    vectors = [model.wv[word] for word in words if word in model.wv]
    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)

df["vector"] = df["tokenized"].apply(lambda x: get_mean_vector(x, model))
``` 

ä¸ºä»€ä¹ˆé‡‡ç”¨è¯å‘é‡åŒ–ï¼Ÿ

è¯å‘é‡å¯ä»¥æ•æ‰è¯­ä¹‰ä¿¡æ¯ï¼Œä½¿å¾—æ–‡æœ¬æ•°æ®èƒ½å¤Ÿåœ¨è¿ç»­çš„ç©ºé—´ä¸­è¡¨ç¤ºï¼Œé€‚ç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¾“å…¥ã€‚

Word2Vec é‡‡ç”¨è¯å…±ç°å…³ç³»æ¥è®­ç»ƒè¯å‘é‡ï¼Œèƒ½è¾ƒå¥½åœ°æ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚

FastText è¿›ä¸€æ­¥è€ƒè™‘äº†å­è¯ä¿¡æ¯ï¼Œå¯¹æ‹¼å†™ç›¸ä¼¼çš„è¯ï¼ˆå¦‚â€œæ¶¨åœâ€å’Œâ€œæ¶¨åœæ¿â€ï¼‰èƒ½æ›´å¥½åœ°å»ºæ¨¡ï¼Œé€‚ç”¨äºå¤„ç†è‚¡æ°‘è¯„è®ºä¸­çš„å¤šå˜è¡¨è¾¾ã€‚

#### æ—¶é—´è¡°å‡åŠ æƒå»ºæ¨¡å¸‚åœºæƒ…ç»ª

ç”±äºè‚¡æ°‘è¯„è®ºçš„å½±å“éšæ—¶é—´è¡°å‡ï¼Œæˆ‘ä»¬å¼•å…¥ **æŒ‡æ•°è¡°å‡åŠ æƒ** æ¥è®¡ç®—æ¯æ—¥å¸‚åœºæƒ…ç»ªå‘é‡ã€‚å…¬å¼å¦‚ä¸‹ï¼š

$$
w_{time}(t) = e^{-\lambda \cdot (T - t)}
$$

å…¶ä¸­ï¼š

- $ w_{time}(t) $ æ˜¯æ—¶é—´è¡°å‡æƒé‡ï¼›
- $ T $ æ˜¯ç›®æ ‡æ—¥æœŸï¼Œ$ t $ æ˜¯è¯„è®ºçš„å‘å¸ƒæ—¶é—´ï¼›
- $ \lambda $ æ˜¯è¡°å‡ç‡ï¼Œå†³å®šäº†è¿‡å»è¯„è®ºçš„é‡è¦æ€§ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨ **è¯„è®ºçš„æµè§ˆé‡** ä½œä¸ºæƒé‡ï¼Œå¼ºè°ƒé«˜æµè§ˆé‡çš„è¯„è®ºåœ¨å¸‚åœºæƒ…ç»ªä¸­çš„å½±å“ã€‚æœ€ç»ˆçš„åŠ æƒæƒ…ç»ªå‘é‡è®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š

$$
\mathbf{V}_{final} = \frac{\sum_i w_{final}^i \cdot \mathbf{V}_i}{\sum_i w_{final}^i}
$$

å…¶ä¸­ï¼š

- $ w_{final}^i = w_{time}^i \times views^i $ æ˜¯ç»¼åˆæƒé‡ï¼›
- $ \mathbf{V}_i $ æ˜¯è¯„è®ºçš„è¯å‘é‡ï¼›
- ç»“æœå‘é‡é‡‡ç”¨**æŒ‡æ•°ç§»åŠ¨å¹³å‡æ³•**è¿›è¡Œå¹³æ»‘ï¼Œä»¥ç¡®ä¿è¿ç»­æ€§ã€‚

```python
lambda_decay = 0.1
alpha = 0.9
previous_vector = None
results = []

for target_date in date_range:
    past_comments = df[df["date"] <= target_date].copy()
  
    if not past_comments.empty:
        past_comments["time_diff"] = (target_date - past_comments["date"]).dt.total_seconds() / 3600
        past_comments["w_time"] = np.exp(-lambda_decay * past_comments["time_diff"])
        past_comments["w_final"] = past_comments["w_time"] * past_comments["views"]
      
        weighted_vectors = np.array([past_comments["w_final"].iloc[i] * past_comments["vector"].iloc[i] for i in range(len(past_comments))])
        final_vector = np.sum(weighted_vectors, axis=0) / np.sum(past_comments["w_final"])
      
        previous_vector = alpha * final_vector + (1 - alpha) * previous_vector if previous_vector is not None else final_vector
    else:
        final_vector = previous_vector if previous_vector is not None else np.zeros(model.vector_size)
  
    results.append([target_date] + final_vector.tolist())
```

##### **ä¸ºä½•ä½¿ç”¨æ—¶é—´è¡°å‡åŠ æƒï¼Ÿ**

- **æ›´è´´è¿‘å¸‚åœºååº”**ï¼šè¿‘æœŸçš„è¯„è®ºå¯¹å¸‚åœºæƒ…ç»ªçš„å½±å“æ›´å¤§ï¼Œå†å²è¯„è®ºå½±å“é€æ¸å‡å¼±ã€‚
- **ç»“åˆæµè§ˆé‡æƒé‡**ï¼Œç¡®ä¿å¸‚åœºæ›´å…³æ³¨çš„è¯„è®ºå¯¹æƒ…ç»ªå‘é‡çš„è´¡çŒ®æ›´é«˜ã€‚

---

#### ä½¿ç”¨ UMAP é™ç»´æƒ…ç»ªå‘é‡

æˆ‘ä»¬é‡‡ç”¨ **UMAPï¼ˆUniform Manifold Approximation and Projectionï¼‰** è¿›è¡Œé™ç»´ï¼Œå°†é«˜ç»´è¯å‘é‡æ˜ å°„åˆ°äºŒç»´ç©ºé—´ï¼Œä»¥ä¾¿ç”¨äºæ—¶åºé¢„æµ‹ã€‚

```python
from umap import UMAP

vector_dim = model.vector_size
columns = ["date"] + [f"vector_{i}" for i in range(vector_dim)]
result_df = pd.DataFrame(results, columns=columns)

umap_model = UMAP(n_components=2, random_state=42)
umap_vectors = umap_model.fit_transform(result_df.iloc[:, 1:])  # åªå¯¹å‘é‡éƒ¨åˆ†é™ç»´

result_df["umap_1"], result_df["umap_2"] = umap_vectors[:, 0], umap_vectors[:, 1]
```

##### **UMAP é™ç»´çš„ä½œç”¨åŠäºŒç»´å‘é‡çš„è§£é‡Š**

- **umap_1ï¼šå¸‚åœºæƒ…ç»ªçš„æç«¯å˜åŒ–ç»´åº¦**ï¼Œä¸»è¦åæ˜ å¸‚åœºæƒ…ç»ªçš„æ³¢åŠ¨ç¨‹åº¦ï¼ˆæ­£è´Ÿé¢æƒ…ç»ªçš„å‰§çƒˆç¨‹åº¦ï¼‰ã€‚
- **umap_2ï¼šå¸‚åœºæƒ…ç»ªçš„è¶‹åŠ¿ç»´åº¦**ï¼Œè¡¡é‡å¸‚åœºæƒ…ç»ªçš„æŒç»­æ€§å˜åŒ–ï¼ˆæ­£é¢æˆ–è´Ÿé¢æƒ…ç»ªçš„æŒç»­æ—¶é—´ï¼‰ã€‚

---

## ğŸ§‘â€ğŸ’» æ¨¡å‹æ–¹æ³•

### 1. å•æ¨¡å‹
- çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰  
- å²­å›å½’ï¼ˆRidgeï¼‰  
- éšæœºæ£®æ—ï¼ˆRandomForestï¼‰  
- æ¢¯åº¦æå‡æ ‘ï¼ˆGBDTï¼‰  
- XGBoost  

### 2. é›†æˆç­–ç•¥
- Mean å¹³å‡é›†æˆ  
- Median ä¸­ä½æ•°é›†æˆ  
- Max æœ€å¤§å€¼é›†æˆ  
- Min æœ€å°å€¼é›†æˆ  
- Weighted åŠ æƒé›†æˆï¼ˆæƒé‡ç¤ºä¾‹ï¼š`[5, 3, 2, 2, 1]`ï¼‰

æœ€ç»ˆé€‰æ‹©éªŒè¯é›† RMSE æœ€å°çš„é›†æˆæ–¹æ³•ä½œä¸ºé¢„æµ‹æ¨¡å‹ã€‚

---

## ğŸ“ˆ å®éªŒç»“æœ

### åç†™ç”Ÿç‰©
- æµ‹è¯•é›†é¢„æµ‹ RMSE: **0.012137**  
- æ­£å¯¹æ•°æ”¶ç›Šç‡å æ¯”: **0.5167**  
- è´Ÿå¯¹æ•°æ”¶ç›Šç‡å æ¯”: **0.4833**  
- æ¶¨è·Œæ–¹å‘é¢„æµ‹å‡†ç¡®ç‡: **0.7000**  
- é¢„æµ‹æ¶¨å‡†ç¡®ç‡ï¼ˆç²¾ç¡®ç‡ï¼‰: **0.8824**  
- é¢„æµ‹è·Œå‡†ç¡®ç‡ï¼ˆç²¾ç¡®ç‡ï¼‰: **0.6279**  
- å¬å›ç‡: **0.9310**

![åç†™ç”Ÿç‰©_æ··åˆé›†æˆé¢„æµ‹è¡¨ç°](688363.png)

---

### ä¸­èŠ¯å›½é™…
- æµ‹è¯•é›†é¢„æµ‹ RMSE: **0.013183**  
- æ­£å¯¹æ•°æ”¶ç›Šç‡å æ¯”: **0.4412**  
- è´Ÿå¯¹æ•°æ”¶ç›Šç‡å æ¯”: **0.5441**  
- æ¶¨è·Œæ–¹å‘é¢„æµ‹å‡†ç¡®ç‡: **0.7500**  
- é¢„æµ‹æ¶¨å‡†ç¡®ç‡ï¼ˆç²¾ç¡®ç‡ï¼‰: **0.6944**  
- é¢„æµ‹è·Œå‡†ç¡®ç‡ï¼ˆç²¾ç¡®ç‡ï¼‰: **0.8125**  
- å¬å›ç‡: **0.7027**  

![ä¸­èŠ¯å›½é™…_æ··åˆé›†æˆé¢„æµ‹è¡¨ç°](688981.png)

---

## âš™ï¸ ä½¿ç”¨æ–¹æ³•

### 1. ç¯å¢ƒé…ç½®
è¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š
```bash
pip install numpy pandas matplotlib scikit-learn xgboost
```

### 2. ç¯å¢ƒé…ç½®

é¡¹ç›®ä½¿ç”¨ Excel æ–‡ä»¶ä½œä¸ºæ•°æ®è¾“å…¥ï¼š

```
688981_v3.xlsx
688363_v3.xlsx
```

å…¶ä¸­åŒ…å«æ‰€æœ‰ç‰¹å¾ä¸ç›®æ ‡å˜é‡ **å¯¹æ•°æ”¶ç›Šç‡**ã€‚

### 3. è¿è¡Œæ–¹å¼

è¿›å…¥é¡¹ç›®æ–‡ä»¶å¤¹ï¼š

```
The-hybrid-ensemble-model-predicts-the-return-rate-of-stocks-on-the-STAR-Market
```

ç›´æ¥è¿è¡Œ Jupyter Notebook æ–‡ä»¶ï¼š

```
Hybrid_Ensemble_Model.ipynb
```

åœ¨ Notebook ä¸­å¯é€æ­¥æ‰§è¡Œä»£ç ï¼Œå®Œæˆæ•°æ®å¤„ç†ã€æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹ã€‚

### ğŸ“Š å…¬å¸å¯¹æ¯”è¡¨

| æŒ‡æ ‡ | åç†™ç”Ÿç‰© | ä¸­èŠ¯å›½é™… |
|------|--------|----------------|
| æµ‹è¯•é›†é¢„æµ‹ RMSE | 0.012137 | 0.013183 |
| æ­£å¯¹æ•°æ”¶ç›Šç‡å æ¯” | 0.5167 | 0.4412 |
| è´Ÿå¯¹æ•°æ”¶ç›Šç‡å æ¯” | 0.4833 | 0.5441 |
| æ¶¨è·Œæ–¹å‘é¢„æµ‹å‡†ç¡®ç‡ | 0.7000 | 0.7500 |
| é¢„æµ‹æ¶¨å‡†ç¡®ç‡ï¼ˆç²¾ç¡®ç‡ï¼‰ | 0.8824 | 0.6944 |
| é¢„æµ‹è·Œå‡†ç¡®ç‡ï¼ˆç²¾ç¡®ç‡ï¼‰ | 0.6279 | 0.8125 |
| å¬å›ç‡ | 0.9310 | 0.7027 |

---

## ğŸ“Œ æ€»ç»“ä¸å±•æœ›
- ä¸¤å®¶å…¬å¸å®éªŒç»“æœè¡¨æ˜ï¼Œ**æ··åˆé›†æˆæ¨¡å‹**åœ¨ä¸åŒè‚¡ç¥¨ä¸Šéƒ½èƒ½å–å¾—è¾ƒå¥½çš„é¢„æµ‹è¡¨ç°ï¼›  
- åç†™ç”Ÿç‰© åœ¨ **é¢„æµ‹æ¶¨ï¼ˆç²¾ç¡®ç‡ï¼‰** å’Œ **å¬å›ç‡** ä¸Šè¡¨ç°æ›´ä¼˜ï¼›  
- ä¸­èŠ¯å›½é™… åœ¨ **æ¶¨è·Œæ–¹å‘é¢„æµ‹å‡†ç¡®ç‡** å’Œ **é¢„æµ‹è·Œçš„ç²¾ç¡®ç‡** ä¸Šæ›´å…·ä¼˜åŠ¿ï¼›  
- è¡¨æ˜æ¨¡å‹å¯¹ä¸åŒè‚¡ç¥¨å¯èƒ½æœ‰ä¸åŒåå¥½ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–é›†æˆç­–ç•¥ï¼›  
- æœªæ¥ç ”ç©¶æ–¹å‘ï¼š  
  - å¼•å…¥æ›´å¤š **æ·±åº¦å­¦ä¹ æ—¶åºæ¨¡å‹**ï¼›  
  - å¢åŠ  **å®è§‚ç»æµä¸è·¨å¸‚åœºå› å­**å’Œæ›´å¤š**é«˜é¢‘æ•°æ®ç‰¹å¾**ï¼›  
  - ç ”ç©¶ **åŠ¨æ€æƒé‡é›†æˆ** æå‡ç¨³å¥æ€§ã€‚
  - å¯¹æƒ…ç»ªå› å­è¿›è¡Œæ›´ç²¾ç»†åŒ–å»ºæ¨¡  

---